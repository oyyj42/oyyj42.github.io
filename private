---
layout: post
title: 无线高清图传项目回顾
category: thinking
---



第一次作为负起责任主导一个项目，从预研到结项导入产品，时间为2015年12月到2016年7月，这里回顾和整理一下，为了简洁和突出重点，这里只包括主要思路和过程，暂不涉及具体代码。

----

这是用在某行业一个远距离低延时无线视频传输项目。需要解决的核心问题是***低延时、流畅清晰和远距离***。

----


##1.接收端
接手这个项目的前两个月，负责安卓端接收和解码。技术框架如下：

![]({{site.baseurl}}/img/video_tx/1.png)

----
###1.1 Usb驱动和接收

驱动是现成的，只是jni编译调用就好了。

Android的USB Host的接口，直接封装linux的usb驱动，主要有以下3个api：


 1. 控制传输  ControlTransfer
 2. 块传输 BulkTransfer
 3. 同步传输（ USBRequest.queue()发送，UsbRequest.RequestWait()接收 ）
 

饿，除了上述3种之外，还有一种中断传输，不知道为什么android 的api忽略掉了。


由于原始视频流格式是Mpeg-TS,拿到原始数据的时候，顺便统计丢包率和错包率，和数据量。

---

### 1.2 Mpeg-TS得到H.264
接下来，Mpeg-TS 转 H.264 ，的到H.264的裸流（也叫ES流），再从裸流中分拆出 I,P,B 帧，由于B帧需要参考前后的帧，对延时很不利，所以忽略。

解析H.264格式播放，主要通过NAL头和NALU解析出每个NALU的格式。每帧帧头格式为：

```
00 00 01 NALU

```

或者

```
00 00 00 01 NALU

```

只要遍历原始数据，找到帧头后查看第4字节也就是NALU的格式即可。  以下为 NALU & 0x1f 的取值

| 后5位取值 | 类型| 描述 |
| :----------: | :----------:| :------------: |
|       1      |    IDR      |      I帧        |
|       2      |    SLICE A  |      编码条带数据分割块A       |
|       3      |    SLICE B  |      编码条带数据分割块B       |
|       4      |    SLICE C  |      编码条带数据分割块C       |
|       5      |    P        |  参考帧，也就是p帧 |
|       6      |    SEI      |   增强信息        |
|       7      |    SPS      |  序列参考集       |
|       8      |    pps      |  图像参考集       |


最简单来讲，只需要提取出 SPS，PPS， I帧， P帧就可以流畅播放了。



--- 

### 1.3 解码显示

#### 1.3.1 FFmpeg软解

FFmpeg的平台兼容性比较好，所有优先尝试。这里用的是armeabi-v7a。

需要注意的是，android平台下调用ffmpeg的api，要在linux下交叉编译(Cross-compile)，但千万避免在windows下用虚拟机跑linux，真心慢。

接下来jni调用ffmpeg解码，可以指定输出的格式，这里用到的是YUV420P，因为每个像素大小仅RGBA格式的2/3。内存能省就省啊。



得到的yuv数据，显示有2个思路。



+ 思路1，通过GLSurfaceView来做颜色空间转换和渲染显示。 参考：[stackoveflow的讨论](http://stackoverflow.com/questions/22456884/how-to-render-androids-yuv-nv21-camera-image-on-the-background-in-libgdx-with-o/22456885#22456885)

+ 思路2， 参考framework中的AwesomePlayer，里面利用AwesomeLocalRenderer/AwesomeRemoteRenderer来实现解码出来的数据显示，这个效率应该非常高，但是平台的关联性会增加很多。
	
	


思路1实践的时候，尝试过的平台包括小米平板，小米3，锤子T1, 华为荣耀6,TCL摸摸哒等等.

后来发现ffmpeg软解 720P（1280*720）,30帧，4Mbps码流比较吃力，小米平板和小米3，荣耀6，锤子T1都算能勉强流畅播放，但发热和耗电也很惊人。

需要强调的是，FFmpeg初始化选项中，可以指定cpu数量为n，内部会有n+1帧的缓冲区，即解码出来的是n+1帧之前的数据了。但带来的延时也是 n+1 倍，所以低延时解码不适合。


而在目标平台（RK平台的cpu，性能逊于15年主流手机）上，用1个cpu解码时，I帧解码需要100多ms，p帧20-40ms，所以会有很明显的卡顿。由于除了播放视频之外，其他cpu还需要跑其他繁重的业务，至此只能放弃ffmpeg了 。


### 1.3.2 MediaCodec

MediaCodec需要Android 4.1+版本。

具体使用方法可以查看[官网文档](http://www.android-doc.com/reference/android/media/MediaCodec.html)，网上也有很多人讨论 。

这里着重讨论它的效率，测试条件是 *** 720P,Gop=10,30fps，4Mbps *** 的码流,解码1800帧 ,平台是高通8916，一个比较弱的平台。
![]({{site.baseurl}}/img/video_tx/2_edit.png)


可以看出，在gpu比较弱的平台，解码延时主要集中在66ms以内，勉强可以接受了。当时做过非正式测试，主流手机33ms以内是没问题的。


后来发现，解码延时和分辨率，码流和GOP有很大关系，而且是一个比较系统性的问题，调优的时候需要从编码、传输、解码统筹考虑，后续不断优化后，已经可以实现流畅的播放720p，30fps，4Mbps码流了。


## 2. 传输端优化

其实编码和解码端的延时可以很容易确认，延时往往来自于传输阶段，例如用 rtmp,接收缓冲一般200ms－1s，加上tcp/udp的网络延时，例如RTT，可惜目前方向不在这，没有做太多研究。


这里的项目是用专用的硬件做无线传输，细节不透露了。但做的时候发现会偶尔卡顿，后来查了好久才发现是传输端问题，抓取统计表如下：(Mac下 用python画的图，Retina屏幕分辨率太高，压缩后请将就看吧)

不同的gop，相同码流下的传输延时：
![]({{site.baseurl}}/img/video_tx/mbps_edit.jpg)




相同GOP，不同码流下传输延时：
![]({{site.baseurl}}/img/video_tx/gop_edit.jpg)




后来通过计算，得出来最优的传输带宽，从而能最高效率利用传输带宽来传输尽量多的数据。最终传输时间优化到了 60-80ms，平均解码时间40ms，编码时间66ms，再加上中间的各种缓冲，系统延时在200-250ms，距离2公里以上,呵呵，已经行业领先了。


## 3.总结、展望和吐槽


从莫名其妙被拉进项目，到成为主导者推进，期间鬼知道都经历了什么。只是和硬件结合的项目，投板打样调试周期真多很长，很需要耐心。


很有兴趣做网络视频流的优化，包括其中了解过有一种封装在udp协议之上的udt协议，专门用于在恶劣带宽下传输大量数据，例如大量的气象数据传输。有兴趣的请参考：


[http://udt.sourceforge.net](http://udt.sourceforge.net)

---

最近看到有的直播技术分享时，用1080p的分辨率，数据量200-300kB/s,算下来码流1.5Mhz－2Mhz，其实当时测试过，影响视频画质的主要是***码流***，跟分辨率真心关系不大。

曾经做过测试（拿安防摄像机的专用测试图），同等码流下，480P,720P和1080P的分辨率，其实细节还原度差别不大的。

